{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential (Predictive) Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "* Usually happens that the differences between consecutive PCM (Pulse-Code Modulation) [[A.V. Oppenheim, 1999](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=oppenheim+discrete+time+signal+processing&btnG=&oq=Oppenheim)] samples\n",
    "\n",
    "  $$\n",
    "    e[n] = s[n] - s[n-1],\n",
    "  $$\n",
    "\n",
    "  tend to have a smaller variance (and therefore, smaller entropy) than the original ($s$) ones\n",
    "\n",
    "  $$\n",
    "    H(e) \\leq H(s),\n",
    "  $$\n",
    "\n",
    "  which potentially provides better lossless compression ratios for $e$ than for $s$.\n",
    "\n",
    "* Finally, notice that by definition, if a source of \"noise\" (such as the quantization noise) has not been introduced during the encoding process, differential encoding is a fully reversible process:\n",
    "\n",
    "  $$\n",
    "    s[n] = e[n] + s[n-1].\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "* Compute $e$ signal for the [Jfk_berlin_address_high.ogg](https://upload.wikimedia.org/wikipedia/commons/3/3a/Jfk_berlin_address_high.ogg) $s$ signal and plot the probability density function (also known as the histogram) of $e$. Note: use Python and Matplotlib, and insert the code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPCM (Differential PCM) [[Cutler, 1952]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=%22Differential+quantization+of+communication+signals%22&btnG=)\n",
    "\n",
    "* Signals sampled at Nyquist rate exhibit correlation between consecutive samples. Therefore, the variance of the first difference\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\text{Var}(\\{s[n+1]-s[n]\\}_{n=0}^{N-1})\n",
    "  \\end{equation}\n",
    "  \n",
    "  will be smaller than the variance of the signal itself\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\text{Var}(\\{s[n+1]-s[n]\\}_{n=0}^{N-1}) < \\text{Var}(s) = \\sigma_s^2.\n",
    "  \\end{equation}\n",
    "\n",
    "* In general, DPCM consist in computing the residual (error signal):\n",
    "\n",
    "  \\begin{equation}\n",
    "    e[n] = s[n] - \\hat{s}[n]\n",
    "  \\end{equation}\n",
    "  \n",
    "  where for the simplest case:\n",
    "  \n",
    "  \\begin{equation}\n",
    "    \\hat{s}[n] = s[n-1]\n",
    "  \\end{equation}\n",
    "\n",
    "  <img src=\"figs/DPCM.png\" width=\"400\">\n",
    "\n",
    "* Used in the [G.726 standard](https://en.wikipedia.org/wiki/G.726)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADPCM (Adaptive DPCM)\n",
    "\n",
    "* The predictor can change its behaviour depending on the characteristics of $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward adaptive prediction\n",
    "\n",
    "* Splits the input into blocks, optimizes the predictor for each block and sends in each block que predictor coefficients as side information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal prediction based on Weiner-Hopf optimization\n",
    "\n",
    "* The predictor can be any structure capable of producing a signal\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\hat{s} \\approx s.\n",
    "  \\end{equation}\n",
    "\n",
    "* In the case of using LPC (Linear Predictive Coding), where\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\hat{s}[n] = \\sum_{i=1}^M a_i s[n-i],\n",
    "  \\end{equation}\n",
    "  \n",
    "  $\\{a_i\\}$ are the LPC coefficients and $M$ is the predictor order.\n",
    "  \n",
    "* Coeffs $\\{a_i\\}$ can be found when they minimize the variance of the residue (error signal)\n",
    "  \n",
    "  \\begin{equation}\n",
    "    \\delta_e^2 = \\text{E}\\big[(s[n]-\\sum_{i=1}^M a_i s[n-i])^2\\big].\n",
    "  \\end{equation}\n",
    "  \n",
    "* For this, it must me hold that\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\left.\n",
    "      \\begin{array}{c}\n",
    "        \\frac{\\text{d}\\delta_e^2}{\\text{d}a_1} = -2\\text{E}\\Big[\\big(s[n]-\\displaystyle\\sum_{i=1}^M a_i s[n-i]\\big)s[n-1]\\Big] = 0 \\\\\n",
    "        \\frac{\\text{d}\\delta_e^2}{\\text{d}a_2} = -2\\text{E}\\Big[\\big(s[n]-\\displaystyle\\sum_{i=1}^M a_i s[n-i]\\big)s[n-12\\Big] = 0 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{\\text{d}\\delta_e^2}{\\text{d}a_M} = -2\\text{E}\\Big[\\big(s[n]-\\displaystyle\\sum_{i=1}^M a_i s[n-i]\\big)s[n-M]\\Big] = 0 \n",
    "      \\end{array}\n",
    "    \\right\\rbrace.\n",
    "  \\end{equation}\n",
    "  \n",
    "* Appying expectations\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\left.\n",
    "      \\begin{array}{c}\n",
    "        \\displaystyle\\sum_{i=1}^M a_i \\text{R}_{ss}(i-1) = \\text{R}_{ss}(1) \\\\\n",
    "        \\displaystyle\\sum_{i=1}^M a_i \\text{R}_{ss}(i-2) = \\text{R}_{ss}(2) \\\\\n",
    "        \\vdots\\\\\n",
    "        \\displaystyle\\sum_{i=1}^M a_i \\text{R}_{ss}(i-N) = \\text{R}_{ss}() \\\\\n",
    "      \\end{array}\n",
    "    \\right\\rbrace,\n",
    "    \\tag{Weiner-Hopf equations}\n",
    "    \\label{Weiner-Hopf equations}\n",
    "  \\end{equation}\n",
    "  \n",
    "  where $\\text{R}_{ss}(k)$ is the autorrelation function of $s$, defined as:\n",
    "  \n",
    "  \\begin{equation}\n",
    "    \\text{R}_{ss}(k) = \\text{E}\\big[s[n]s[n-k]\\big] = \\frac{1}{M-k}\\sum_{i=1}^{M-k} s[i]s[i+k].\n",
    "    \\tag{autocorrelation}\n",
    "    \\label{autocorrelation}\n",
    "  \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "For each $M=\\{1,2,3\\}$, determine the optimal predictor for the audio [Jfk_berlin_address_high.ogg](https://upload.wikimedia.org/wikipedia/commons/3/3a/Jfk_berlin_address_high.ogg). Next, compute the variance of $e$ for each prediction order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward adaptive prediction\n",
    "\n",
    "* Both, the encoder and the decoder can constinuosly adapt the prediction to the characteristics of $s$, using the information that is available at the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive prediction based on Least Mean Squared (LMS) algorithm\n",
    "\n",
    "* As we know,\n",
    "\n",
    "  \\begin{equation}\n",
    "    e[n] = s[n] = \\hat{s}[n] = s[n] - \\sum_{i=1}^M a_is[n-i].\n",
    "  \\end{equation}\n",
    "  \n",
    "* Our objective is to minimize $e[n]$, which is the same that minimizing the energy of the prediction error $e^2$, by controlling the $\\{a_i\\}_{i=1}^M$ coefficients.\n",
    "\n",
    "* Suppose that we can control iteratively these coefficients, by incrementing or decrementing them with a proportionality constant $\\alpha$ (the larger $\\alpha$, the faster the convergence and viceversa). Let's denote $a_i^{[n]}$ the value of coeff $a_i$ at iteration $n$ of the algorithm. If happens, for example, that\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\frac{\\text{d}e^2[n]}{\\text{d}a_i} < 0\n",
    "  \\end{equation}\n",
    "  \n",
    "  (the squared prediction error for iteration $n$ has been increased), then the prediction $\\hat{s}[n]$ has been too small, and viceversa. Let's define the iterative control of $\\{a_i\\}_{i=1}^M$ coefficients as\n",
    "  \n",
    "  \\begin{equation}\n",
    "    \\Big\\{a_i^{[n-1]} = a_i^{[n]} - \\alpha\\frac{\\text{d}e^2[n]}{\\text{d}a_i}\\Big\\}_{i=1}^M\n",
    "    \\tag{LMS_coeffs_control}\n",
    "  \\end{equation}\n",
    "  \n",
    "* Applyig derivatives, for example, for $a_i$ we obtain that, \n",
    "\n",
    "  \\begin{equation}\n",
    "    \\frac{\\text{d}e^2[n]}{\\text{d}a_i} = \\frac{\\text{d}\\Big(s[n] - \\sum_{i=1}^M a_is[n-i]\\Big)^2}{\\text{d}a_i} = -2\\Big(s[n] - \\sum_{i=1}^M a_is[n-i]\\Big)^2\\Big)s[n-i] = -2e[n]s[n-i].\n",
    "    \\end{equation}\n",
    "    \n",
    "* Substituting this expression in Eqs. LMS_coeffs_control, we get that\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\Big\\{a_i^{[n-1]} = a_i^{[n]} +2\\alpha e[n]s[n-i]\\Big\\}_{i=1}^M.\n",
    "    \\tag{LMS_coeffs_control_final}\n",
    "  \\end{equation}\n",
    "\n",
    "* Notice that, in a backward adaptive prediction scheme, both $e[n]$ and $s[n-1]$ are known at iteration $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lossy DPCM [[Cutler, 1952]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=%22Differential+quantization+of+communication+signals%22&btnG=)\n",
    "\n",
    "<img src=\"figs/QDPCM.png\\\" width=800>\n",
    "\n",
    "* The prediction error is quantized to decrease the variance:\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\tilde{e}[n] = Q(e[n])\n",
    "  \\end{equation}\n",
    "  \n",
    "  where\n",
    "  \n",
    "  \\begin{equation}\n",
    "    e[n] = s[n] - \\hat{\\tilde{s}}[n]\n",
    "  \\end{equation}\n",
    "  \n",
    "  where\n",
    "  \n",
    "  \\begin{equation}\n",
    "    \\tilde{s}[n] = \\hat{\\tilde{s}}[n] + \\tilde{e}[n].\n",
    "  \\end{equation}\n",
    "\n",
    "* Used in [hybrid (lossy DPCM/transform) video coding](https://en.wikipedia.org/wiki/High_Efficiency_Video_Coding#Video_coding_layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
